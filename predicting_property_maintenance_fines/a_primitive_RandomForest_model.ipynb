{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Background about Blight Violation in Detroit \n",
    "This information is obtained from the link: https://detroitmi.gov/Portals/0/docs/Brochures/DAH/DAH_Citizen_Guide.pdf\n",
    "\n",
    "### What is a Blight Violation? \n",
    "The City of Detroit has ordinances that address how property owners must maintain the exterior of their property. The City issues a blight violation when an owner fails to follow these ordinances. Examples of blight violations that come before the DAH are:\n",
    "* <b>Property Maintenance</b>: Failure to obtain certificate of compliance or rental registration, failure to maintain exterior of property, failure to comply with emergency order, rat harborage and failure to remove snow and ice.\n",
    "* <b>Zoning</b>: Violation of special land use grant, change of use of land without permit, change of use of building without a permit, failure to obtain certificate of maintenance of grant conditions.\n",
    "* <b>Solid Waste & Illegal Dumping</b>: Early or late placement or improper storage of Courville containers, improper set-out during eviction, improper storage of solid, medial or hazardous waste, improper bulk set-out and illegal dumping\n",
    " \n",
    "### Who issues Blight Violation Notices (BVNs)?\n",
    "Blight Violation Notices (BVNs) are written tickets issued by City inspectors, police officers, and other City officials who investigate complaints of blight. Blight violation notices are issued to property owners or those in control of property that is in violation of the City’s anti-blight ordinances. If a blight violation notice is issued, the person or entity in receipt is called a respondent.\n",
    "### What happens when a Blight Violation Notice (BVN) is issued?\n",
    "The written blight violation notice (BVN) received by a respondent will provide a description of the alleged violation and give the hearing date and time. Once a BVN is issued, the following options are available to the respondent who received the BVN:\n",
    "* Admit responsibility and pay the fine and fees before the DAH hearing date; fine is reduced 10% for early payment.\n",
    "* Attend the hearing and contest the blight violation, with or without an attorney.\n",
    "* If a property owner is found responsible at the hearing, the fine and fees imposed must be paid by the hearing date or a 10% penalty is imposed for late payment.\n",
    "\n",
    "### What is the DAH Hearing Process?\n",
    "A respondent who receives a blight violation notice has the right to attend a hearing at the DAH. At the hearing, the respondent may present a defense to the blight violation. DAH hearings are presided over by Administrative Hearing Officers who are licensed Michigan attorneys and independent contractors. At the conclusion of the hearing, the Administrative Hearing Officer will make finding of facts and issue a written Decision and Order and Judgment. A Decision and Order and Judgment issued by the DAH is a state civil judgment and is treated the same as any other state court judgment for enforcement purposes\n",
    "\n",
    "### What if payment is not made?\n",
    "If an individual ignores a blight violation notice and doesn’t appear at the hearing, a Decision and Order and Judgment by Default will be issued finding the respondent responsible for the blight violation. If a respondent fails to pay the amount of the Decision and Order and Judgment, collection actions will be commenced, which may include the garnishment of wages, attachment of bank accounts and assets, and imposition of judgment liens upon real property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "# Part A: Taking a look of the datasets\n",
    "\n",
    "(Detailed examination of the datasets is provided in the script \"introduction_and_data_exploration.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingjinghuang/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/Users/jingjinghuang/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (11,12,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "traindata = pd.read_csv('train.csv',encoding = \"ISO-8859-1\")\n",
    "testdata = pd.read_csv('test.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((250306, 34), (61001, 27))\n",
      "('features of traindata', Index([u'ticket_id', u'agency_name', u'inspector_name', u'violator_name',\n",
      "       u'violation_street_number', u'violation_street_name',\n",
      "       u'violation_zip_code', u'mailing_address_str_number',\n",
      "       u'mailing_address_str_name', u'city', u'state', u'zip_code',\n",
      "       u'non_us_str_code', u'country', u'ticket_issued_date', u'hearing_date',\n",
      "       u'violation_code', u'violation_description', u'disposition',\n",
      "       u'fine_amount', u'admin_fee', u'state_fee', u'late_fee',\n",
      "       u'discount_amount', u'clean_up_cost', u'judgment_amount',\n",
      "       u'payment_amount', u'balance_due', u'payment_date', u'payment_status',\n",
      "       u'collection_status', u'grafitti_status', u'compliance_detail',\n",
      "       u'compliance'],\n",
      "      dtype='object'))\n",
      "('features of testdata', Index([u'ticket_id', u'agency_name', u'inspector_name', u'violator_name',\n",
      "       u'violation_street_number', u'violation_street_name',\n",
      "       u'violation_zip_code', u'mailing_address_str_number',\n",
      "       u'mailing_address_str_name', u'city', u'state', u'zip_code',\n",
      "       u'non_us_str_code', u'country', u'ticket_issued_date', u'hearing_date',\n",
      "       u'violation_code', u'violation_description', u'disposition',\n",
      "       u'fine_amount', u'admin_fee', u'state_fee', u'late_fee',\n",
      "       u'discount_amount', u'clean_up_cost', u'judgment_amount',\n",
      "       u'grafitti_status'],\n",
      "      dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "# check datasets shapes\n",
    "print(traindata.shape, testdata.shape)\n",
    "\n",
    "# check features\n",
    "print('features of traindata', traindata.columns)\n",
    "print('features of testdata', testdata.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Based on analysis of features, these features may be used:\n",
    "( note: both date and string variables will be tested)\n",
    "* inspector_name\n",
    "* violation_street_name\n",
    "* mailing_address_str_name\n",
    "* *ticket_issued_date* (how can we use this to predict future observations? different months may have different propensities for blight?)\n",
    "* *hearing_date* (how can we use this to predict future observations? This seems very relevant !!!)\n",
    "* violation_code\n",
    "* violation_description\n",
    "* disposition\n",
    "* fine_amount (0.0 or othersize)\n",
    "* late_fee\n",
    "* discount_amount\n",
    "* judgment_amount\n",
    "\n",
    "### I finally decided to start with these features for constructing a RandomForest model;\n",
    "* disposition\n",
    "* timegap - a new feature that is constructed based on ticket_issued_date and hearing date\n",
    "* fine_amount \n",
    "* discount_amount\n",
    "* judgment_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " <br>\n",
    " <br>\n",
    "  <br>\n",
    "Next, we're going to... \n",
    " \n",
    " <br>\n",
    " <br>\n",
    "# Build a RandomForest model\n",
    "(note that I have optimized parameters for this model and details are in the script \"introduction_and_data_exploration.ipynb\")\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " ### 1. Loading and processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# read the two datasets\n",
    "traindata = pd.read_csv('train.csv',encoding = \"ISO-8859-1\")\n",
    "testdata = pd.read_csv('test.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "# add one more feature\n",
    "columns_to_keep = ['disposition','fine_amount','late_fee', 'discount_amount',\n",
    "                   'judgment_amount','hearing_date', 'ticket_issued_date', 'compliance']\n",
    "columns_to_keep_test = ['disposition','fine_amount','late_fee', 'discount_amount',\n",
    "                        'judgment_amount','hearing_date', 'ticket_issued_date']\n",
    "\n",
    "traindata = traindata[columns_to_keep]\n",
    "testdata = testdata[columns_to_keep_test]\n",
    "\n",
    "# reduce memory usage by casting\n",
    "for i in range(len(traindata.columns)):\n",
    "    if len(traindata[traindata.columns[i]].unique()) < 250:\n",
    "        traindata[traindata.columns[i]] = traindata[traindata.columns[i]].astype('category')\n",
    "\n",
    "# Double check and drop any of the columns and rows that contains NAN\n",
    "traindata = traindata.dropna(axis=1,how='all') # drop columns that contain only Nans\n",
    "traindata = traindata.dropna(axis=0,how='any') # drop rows that contain at least one Nan\n",
    "traindata = traindata[traindata['compliance'].notnull()]\n",
    "\n",
    "\n",
    "# to make sure the disposition variable is same in both training and testing sets \n",
    "traindata['disposition'] = traindata.disposition.astype(str)\n",
    "testdata['disposition'] = testdata.disposition.astype(str)\n",
    "\n",
    "\n",
    "# there are few instances in the testdata set that contain strange values of the 'diposition' variable;\n",
    "# these values are not present in the traindata set, and therefore, I replaced these values with the values \n",
    "# from the traindata set\n",
    "testdata['disposition'].replace(['Responsible (Fine Waived) by Admis'], 'Responsible by Admission',inplace=True)\n",
    "testdata['disposition'].replace(['Responsible - Compl/Adj by Default'], 'Responsible by Default',inplace=True)\n",
    "testdata['disposition'].replace(['Responsible - Compl/Adj by Determi'], 'Responsible by Determination',inplace=True)\n",
    "testdata['disposition'].replace(['Responsible by Dismissal'], 'Responsible (Fine Waived) by Deter',inplace=True)\n",
    "\n",
    "\n",
    "# process the date time variables\n",
    "traindata['hearing_date'] = pd.to_datetime(traindata['hearing_date'])\n",
    "traindata['ticket_issued_date'] = pd.to_datetime(traindata['ticket_issued_date'])\n",
    "testdata['hearing_date'] = pd.to_datetime(testdata['hearing_date'])\n",
    "testdata['ticket_issued_date'] = pd.to_datetime(testdata['ticket_issued_date'])\n",
    "\n",
    "\n",
    "# compute a new variable date time gap\n",
    "traindata['time_gap'] = traindata['hearing_date'].subtract(traindata['ticket_issued_date'])\n",
    "traindata['time_gap'] = traindata['time_gap'].dt.days\n",
    "traindata.drop(['hearing_date','ticket_issued_date'],axis = 1,inplace = True)\n",
    "testdata['time_gap'] = testdata['hearing_date'].subtract(testdata['ticket_issued_date'])\n",
    "testdata['time_gap'] = testdata['time_gap'].dt.days\n",
    "testdata.drop(['hearing_date','ticket_issued_date'],axis = 1,inplace = True)\n",
    "\n",
    "# process and transform string_features to two-digit variables\n",
    "string_features = ['disposition']\n",
    "traindata =  pd.get_dummies(traindata,columns = string_features,drop_first = False)\n",
    "testdata =  pd.get_dummies(testdata,columns = string_features,drop_first = False)\n",
    "\n",
    "\n",
    "y = traindata['compliance']\n",
    "X = traindata.drop('compliance',axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Other quick and simple models as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9284962669739941 0.928369203016561 0.5\n",
      "0.7159445222448417 0.876684872475823 0.8750782952922607\n"
     ]
    }
   ],
   "source": [
    "# some other models as baselines\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy = 'most_frequent').fit(X_train,y_train)\n",
    "ypred = dummy_clf.predict(X_test)\n",
    "\n",
    "print dummy_clf.score(X_test,y_test),dummy_clf.score(X_train,y_train),roc_auc_score(y_test,ypred)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = nbclf.predict(X_test)\n",
    "\n",
    "print roc_auc_score(y_test,y_pred),nbclf.score(X_test, y_test),(nbclf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Run GridSearchCV to attain paramters \n",
    "This was run exclusively by supercomputers, and the section here is shown just for double check!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Here is the piece of the codes that was run on a HPC cluster: <br>\n",
    "----------------------------------------------------------------\n",
    "\\#Number of trees in random forest <br>\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 50)] <br>\n",
    "\n",
    "\\#Minimum number of samples required to split a node <br>\n",
    "min_samples_split = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "\\#Number of features to consider at every split <br>\n",
    "max_features = ['auto', 'sqrt'] <br>\n",
    "\n",
    "\\#Maximum number of levels in tree <br>\n",
    "max_depth = [int(x) for x in np.linspace(2, 110, num = 50)] <br>\n",
    "max_depth.append(None) <br>\n",
    "\n",
    "\\#Create the random grid <br>\n",
    "param_grid =  {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, \n",
    "               'min_samples_split': min_samples_split} \n",
    "\n",
    "reg_RF = RandomForestRegressor() <br> \n",
    "\n",
    "grid_reg_RF = RandomizedSearchCV(estimator = reg_RF, param_distributions = param_grid, \n",
    "                               n_iter = 5000, cv = 3, verbose=2, random_state=42,  \n",
    "                                        n_jobs = -1, scoring='roc_auc')\n",
    "\n",
    "grid_reg_RF.fit(X_train, y_train) <br>\n",
    "\n",
    "print('Grid best parameter (max. accuracy): ', grid_reg_RF.best_params_) <br>\n",
    "print('Grid best score (roc_auc): ', grid_reg_RF.best_score_) <br>\n",
    "\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [80], 'max_features': ['sqrt'], 'min_samples_split': [6], 'max_depth': [8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the random grid\n",
    "param_grid =  {'n_estimators': [80],\n",
    "               'max_features': ['sqrt'],\n",
    "               'max_depth': [8],\n",
    "               'min_samples_split': [6]}\n",
    "\n",
    "\n",
    "reg_RF = RandomForestRegressor()\n",
    "\n",
    "grid_reg_RF = GridSearchCV(reg_RF, param_grid = param_grid, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "grid_reg_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Build Random Forest using the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=160, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.8000819277656926\n",
    "#reg_RF = RandomForestRegressor(n_estimators=80,min_samples_split=6, max_features='sqrt',max_depth=8)\n",
    "\n",
    "# 0.8041931516203397\n",
    "reg_RF = RandomForestRegressor(n_estimators=160,min_samples_split=4, max_features='sqrt',max_depth=8)\n",
    "\n",
    "\n",
    "reg_RF.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120788475310627 0.7944847055722439\n",
      "0.25465366534870093 0.2772064802208417\n"
     ]
    }
   ],
   "source": [
    "ypred_train = reg_RF.predict(X_train)\n",
    "ypred_test = reg_RF.predict(X_test)\n",
    "\n",
    "print roc_auc_score(y_train,ypred_train), roc_auc_score(y_test,ypred_test)\n",
    "print reg_RF.score(X_test,y_test),reg_RF.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. Make predictions for the test.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testdata_full = pd.read_csv('test.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "testdata = testdata.fillna(value=0)\n",
    "test_pred = reg_RF.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(test_pred, index=testdata_full['ticket_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticket_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284932</th>\n",
       "      <td>0.062831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285362</th>\n",
       "      <td>0.013707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285361</th>\n",
       "      <td>0.076736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285338</th>\n",
       "      <td>0.056978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285346</th>\n",
       "      <td>0.072584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "ticket_id          \n",
       "284932     0.062831\n",
       "285362     0.013707\n",
       "285361     0.076736\n",
       "285338     0.056978\n",
       "285346     0.072584"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "# What is RandomForest model? (A brief intro) \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(please refer to the blog: https://towardsdatascience.com/understanding-random-forest-58381e0602d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest is basically **an ensemble (forest) of decision trees**. An example of decision tree is shown as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/decision_tree.jpeg\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "At each node of a decision tree, it tries to split the target group in the way that the resulting groups are as different from each other as possible \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/figure_02.jpeg\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "In a RandomForest model, each decision tree outputs a prediction, and the prediction with the most votes becomes the prediction of the model.\n",
    "\n",
    "The fundamental idea of the RandomForest model is that: a large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. \n",
    "\n",
    "The prerequisites for random forest to perform well are:\n",
    "1. There needs to be some **actual signal in our features** so that models built using those features do better than random guessing.\n",
    "2. The predictions (and therefore the errors) made by the individual trees need to have **low correlations** with each other.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### So, how does Random Forest ensure that decision trees are not correlated?\n",
    "<br>\n",
    "<br>\n",
    "it uses two methods:\n",
    "1. Bagging (Bootstrap Aggregation) — Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly different tree structures. Random forest takes advantage of this by allowing each individual tree to **randomly sample from the dataset with replacement**, resulting in different trees. This process is known as bagging.\n",
    "2. Feature Randomness — each tree in a random forest can pick **only from a random subset of features**. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"imgs/figure_03.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
